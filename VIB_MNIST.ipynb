{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-24T07:20:46.549995200Z",
     "start_time": "2024-12-24T07:20:43.820021900Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from utils import cuda, MIEvaluator\n",
    "from model import VIBencoder, Decoder, Disc, Encoder\n",
    "from numbers import Number\n",
    "from pathlib import Path\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b388fae5a3f254f"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "Cuda = None\n",
    "epoch = 30\n",
    "batch_size = 100\n",
    "lr = 1e-2\n",
    "eps = 1e-9\n",
    "K = 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T07:20:46.565450Z",
     "start_time": "2024-12-24T07:20:46.550991700Z"
    }
   },
   "id": "8ac1294f6b4a8309"
  },
  {
   "cell_type": "markdown",
   "source": [
    "SVIB Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c31163695809535d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class SIB():\n",
    "    def __init__(self, K=10, n=2, beta=1e-3):\n",
    "        super(SIB, self).__init__()\n",
    "        ens = []\n",
    "        for i in range(n):\n",
    "            en = Encoder(K=K, n=2)\n",
    "            ens.append(en)\n",
    "        self.n = n\n",
    "        self.K = K\n",
    "        self.beta = beta\n",
    "        self.encode = nn.ModuleList(ens)\n",
    "        self.decode = Decoder(K)\n",
    "        self.disc = Disc(K, n)\n",
    "        self.w = nn.Parameter(cuda(torch.ones([n, 1]), Cuda))#.cuda()\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        for i, net in enumerate(self.encode):\n",
    "            print(f\"Training encoder {i + 1}\")\n",
    "            if i == 0:\n",
    "                net.train()\n",
    "                self.decode.train()\n",
    "                optim1 = optim.Adam(net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "                optim2 = optim.Adam(self.decode.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "                global_epoch = 0\n",
    "\n",
    "                for e in range(epoch):\n",
    "                    global_epoch += 1\n",
    "                    global_iter = 0\n",
    "                    for idx, (images, labels) in enumerate(dataloader):\n",
    "                        global_iter += 1\n",
    "\n",
    "                        x = Variable(cuda(images, Cuda))\n",
    "                        y = Variable(cuda(labels, Cuda))\n",
    "                        (mu, std), emb = net(x)\n",
    "                        logit = self.decode(emb)\n",
    "\n",
    "                        mu2 = torch.zeros(*mu.size()).to(mu.device)\n",
    "                        std2 = torch.ones(*std.size()).to(std.device)\n",
    "\n",
    "                        class_loss = F.cross_entropy(logit, y)\n",
    "                        info_loss = 0.5 * (2 * (torch.log(std2+1e-8) - torch.log(std+1e-8))+ ((mu - mu2) / std2) ** 2+ (std / std2) ** 2- 1)\n",
    "                        info_loss = info_loss.mean()\n",
    "                        total_loss = class_loss + self.beta * (info_loss)\n",
    "\n",
    "\n",
    "                        optim1.zero_grad()\n",
    "                        optim2.zero_grad()\n",
    "                        total_loss.backward()\n",
    "                        optim1.step()\n",
    "                        optim2.step()\n",
    "\n",
    "            else:\n",
    "                net.train()\n",
    "                self.decode.eval()\n",
    "                self.disc.train()\n",
    "                optim0 = optim.Adam(net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "                optim1 = optim.Adam(self.disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "                global_epoch = 0\n",
    "                Nan = False\n",
    "                for e in range(epoch):\n",
    "                    if Nan:\n",
    "                        break\n",
    "                    global_epoch += 1\n",
    "                    global_iter = 0\n",
    "                    for idx, (images, labels) in enumerate(dataloader):\n",
    "                        global_iter += 1\n",
    "\n",
    "                        x = Variable(cuda(images, Cuda))\n",
    "                        y = Variable(cuda(labels, Cuda))\n",
    "                        (mu, std), emb = net(x)\n",
    "                        logit = self.decode(emb)\n",
    "\n",
    "                        before_mu = cuda(torch.zeros([batch_size, self.K]), Cuda)\n",
    "                        before_std = cuda(torch.zeros([batch_size, self.K]), Cuda)\n",
    "                        for j in range(i):\n",
    "                            self.encode[j].eval()\n",
    "                            (current_mu, current_std), _ = self.encode[j](x)\n",
    "                            before_mu += current_mu\n",
    "                            before_std += current_std\n",
    "                        before_encoding = self.reparametrize_n(before_mu, before_std)\n",
    "\n",
    "                        true = torch.concat((emb, before_encoding), dim=1)\n",
    "                        index1 = torch.randperm(emb.shape[0])\n",
    "                        index2 = torch.randperm(emb.shape[0])\n",
    "                        shuffled1 = emb[index1, :]\n",
    "                        shuffled2 = before_encoding[index2, :]\n",
    "                        false = torch.concat((shuffled1, shuffled2), dim=1)\n",
    "                        true_stat = self.disc(true.detach().clone())\n",
    "                        false_stat = self.disc(false.detach().clone())\n",
    "\n",
    "                        mu2 = torch.zeros(*mu.size()).to(mu.device)\n",
    "                        std2 = torch.ones(*std.size()).to(std.device)\n",
    "\n",
    "                        class_loss = F.cross_entropy(logit, y)\n",
    "                        info_loss = 0.5 * (2 * (torch.log(std2+1e-8) - torch.log(std+1e-8))+ ((mu - mu2) / std2) ** 2 + (std / std2) ** 2- 1)\n",
    "                        info_loss = info_loss.mean()\n",
    "                        total_loss = class_loss + self.beta * (info_loss) - torch.log(true_stat+1e-8).mean()\n",
    "                        if torch.isnan(total_loss):\n",
    "                            Nan = True\n",
    "                            break\n",
    "\n",
    "                        optim0.zero_grad()\n",
    "                        total_loss.backward(retain_graph=True)\n",
    "                        optim0.step()\n",
    "\n",
    "                        #true_stat = self.disc(true.detach().clone())\n",
    "                        #false_stat = self.disc(false.detach().clone())\n",
    "                        disc_loss = -torch.log((1-true_stat)+1e-8).mean()-torch.log(false_stat+1e-8).mean()\n",
    "\n",
    "                        optim1.zero_grad()\n",
    "                        disc_loss.backward()\n",
    "                        optim1.step()\n",
    "\n",
    "        print('Train W')\n",
    "        optim0 = optim.Adam([self.w], lr=lr, betas=(0.5, 0.999))\n",
    "        global_epoch = 0\n",
    "        Nan = False\n",
    "        for e in range(epoch):\n",
    "            if Nan:\n",
    "                break\n",
    "            global_epoch += 1\n",
    "            global_iter = 0\n",
    "            for idx, (images, labels) in enumerate(dataloader):\n",
    "                global_iter += 1\n",
    "\n",
    "                x = Variable(cuda(images, Cuda))\n",
    "                y = Variable(cuda(labels, Cuda))\n",
    "                mu = cuda(torch.zeros([batch_size, self.K]), Cuda)\n",
    "                std = cuda(torch.zeros([batch_size, self.K]), Cuda)\n",
    "                for j in range(self.n):\n",
    "                    self.encode[j].eval()\n",
    "                    (current_mu, current_std), _ = self.encode[j](x)\n",
    "                    mu += current_mu * self.w[j]\n",
    "                    std += current_std * self.w[j]**2\n",
    "                encoding = self.reparametrize_n(mu, std)\n",
    "                logit = self.decode(encoding)\n",
    "\n",
    "                mu2 = torch.zeros(*mu.size()).to(mu.device)\n",
    "                std2 = torch.ones(*std.size()).to(std.device)\n",
    "\n",
    "                class_loss = F.cross_entropy(logit, y)\n",
    "                info_loss = 0.5 * (2 * (torch.log(std2+1e-8) - torch.log(std+1e-8))+ ((mu - mu2) / std2) ** 2+ (std / std2) ** 2- 1)\n",
    "                info_loss = info_loss.mean()\n",
    "                total_loss = class_loss + self.beta * (info_loss)\n",
    "\n",
    "                if torch.isnan(total_loss):\n",
    "                    Nan = True\n",
    "                    break\n",
    "\n",
    "                optim0.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optim0.step()\n",
    "\n",
    "\n",
    "    #def save(self, encode_path, decode_path, w_path):\n",
    "    #    self.encode.load_state_dict(torch.load(encode_path))\n",
    "    #    self.decode.load_state_dict(torch.load(decode_path))\n",
    "    #    self.w = torch.load(w_path)\n",
    "        #s = torch.sum(self.w)\n",
    "        #self.w /= s.detach().clone()\n",
    "\n",
    "    def load(self, encode_path, decode_path, w_path):\n",
    "        self.encode.load_state_dict(torch.load(encode_path))\n",
    "        self.decode.load_state_dict(torch.load(decode_path))\n",
    "        self.w = torch.load(w_path)\n",
    "\n",
    "    def fit(self, dataloader):\n",
    "\n",
    "        def enc(x):\n",
    "            mu = cuda(torch.zeros([batch_size, self.K]), Cuda)\n",
    "            std = cuda(torch.zeros([batch_size, self.K]), Cuda)\n",
    "            for j in range(self.n):\n",
    "                self.encode[j].eval()\n",
    "                (current_mu, current_std), _ = self.encode[j](x)\n",
    "                mu += current_mu * self.w[j]\n",
    "                std += current_std * self.w[j]\n",
    "            emb = self.reparametrize_n(mu, std)\n",
    "            return (mu, std), emb\n",
    "\n",
    "        accuracy = 0\n",
    "        global_iter = 0\n",
    "        for idx, (images, labels) in enumerate(dataloader):\n",
    "            global_iter += 1\n",
    "\n",
    "            x = Variable(cuda(images, Cuda))\n",
    "            y = Variable(cuda(labels, Cuda))\n",
    "            mu = cuda(torch.zeros([batch_size, self.K]), Cuda)\n",
    "            std = cuda(torch.zeros([batch_size, self.K]), Cuda)\n",
    "            for j in range(self.n):\n",
    "                self.encode[j].eval()\n",
    "                (current_mu, current_std), _ = self.encode[j](x)\n",
    "                mu += current_mu * self.w[j]\n",
    "                std += current_std * self.w[j]\n",
    "            logit = self.decode(mu)\n",
    "\n",
    "            prediction = F.softmax(logit, dim=1).max(1)[1]\n",
    "            accuracy += torch.eq(prediction, y).float().mean()\n",
    "\n",
    "        accuracy /= global_iter\n",
    "        print('acc:{:.4f}'\n",
    "                  .format(accuracy.data), end=' ')\n",
    "\n",
    "        mi = MIEvaluator(enc, self.decode, mu.device)\n",
    "        izx_bound = mi.eval_mi_x_z_monte_carlo(dataloader)\n",
    "        izy_bound = mi.eval_mi_y_z_variational_lb(dataloader, 10)\n",
    "\n",
    "        print('IZY:{:.2f} IZX:{:.2f}'\n",
    "                  .format(izy_bound.data, izx_bound.data))\n",
    "\n",
    "        return izx_bound.data, izy_bound.data\n",
    "\n",
    "    def reparametrize_n(self, mu, std, n=1):\n",
    "        def expand(v):\n",
    "            if isinstance(v, Number):\n",
    "                return torch.Tensor([v]).expand(n, 1)\n",
    "            else:\n",
    "                return v.expand(n, *v.size())\n",
    "\n",
    "        if n != 1:\n",
    "            mu = expand(mu)\n",
    "            std = expand(std)\n",
    "\n",
    "        eps = Variable(cuda(std.data.new(std.size()).normal_(), std.is_cuda))\n",
    "\n",
    "        return mu + eps * std\n",
    "\n",
    "    def parameter_counter(self):\n",
    "        a = 0\n",
    "        for p in self.encode.parameters():\n",
    "            l = 1\n",
    "            for j in p.size():\n",
    "                l *= j\n",
    "            a = a + l\n",
    "\n",
    "        b = 0\n",
    "        for p in self.decode.parameters():\n",
    "            l = 1\n",
    "            for j in p.size():\n",
    "                l *= j\n",
    "            b = b + l\n",
    "        return a + b"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T07:20:46.595349700Z",
     "start_time": "2024-12-24T07:20:46.565450Z"
    }
   },
   "id": "63cfcb2dae44a8ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "VIB Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0a62bce96a4700a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class VIB():\n",
    "    def __init__(self, K=10, n=2, beta=1e-3):\n",
    "        super(VIB, self).__init__()\n",
    "        self.n = n\n",
    "        self.K = K\n",
    "        self.beta = beta\n",
    "        self.encoder = VIBencoder(K=K)\n",
    "        self.decoder = Decoder(K=K)\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        global_epoch = 0\n",
    "        optim1 = optim.Adam(self.encoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        optim2 = optim.Adam(self.decoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        for e in range(epoch):\n",
    "            global_epoch += 1\n",
    "            self.encoder.train()\n",
    "            self.decoder.train()\n",
    "            global_iter = 0\n",
    "            for idx, (images, labels) in enumerate(dataloader):\n",
    "                global_iter += 1\n",
    "\n",
    "                x = Variable(cuda(images, Cuda))\n",
    "                y = Variable(cuda(labels, Cuda))\n",
    "                (mu, std), emb = self.encoder(x)\n",
    "                logit = self.decoder(emb)\n",
    "\n",
    "                mu2 = torch.zeros(*mu.size()).to(mu.device)\n",
    "                std2 = torch.ones(*std.size()).to(std.device)\n",
    "\n",
    "                class_loss = F.cross_entropy(logit, y)\n",
    "                info_loss = 0.5 * (2 * (torch.log(std2+1e-8) - torch.log(std+1e-8))+ ((mu - mu2) / std2) ** 2+ (std / std2) ** 2- 1)\n",
    "                info_loss = info_loss.mean()\n",
    "                total_loss = class_loss + self.beta * (info_loss)\n",
    "\n",
    "                optim1.zero_grad()\n",
    "                optim2.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optim1.step()\n",
    "                optim2.step()\n",
    "\n",
    "\n",
    "\n",
    "    #def save(self, encode_path, decode_path, w_path):\n",
    "    #    self.encode.load_state_dict(torch.load(encode_path))\n",
    "    #    self.decode.load_state_dict(torch.load(decode_path))\n",
    "    #    self.w = torch.load(w_path)\n",
    "\n",
    "    def load(self, encode_path, decode_path):\n",
    "        self.encoder.load_state_dict(torch.load(encode_path))\n",
    "        self.decoder.load_state_dict(torch.load(decode_path))\n",
    "\n",
    "    def fit(self, dataloader):\n",
    "        global_epoch = 0\n",
    "        accuracy = 0\n",
    "        for idx, (images, labels) in enumerate(dataloader):\n",
    "            global_epoch += 1\n",
    "            self.encoder.eval()\n",
    "            self.decoder.eval()\n",
    "            x = Variable(cuda(images, Cuda))\n",
    "            y = Variable(cuda(labels, Cuda))\n",
    "            (mu, std), emb = self.encoder(x)\n",
    "            logit = self.decoder(mu)\n",
    "\n",
    "\n",
    "            prediction = F.softmax(logit, dim=1).max(1)[1]\n",
    "            accuracy += torch.eq(prediction, y).float().mean()\n",
    "\n",
    "        accuracy /= global_epoch\n",
    "        print('acc:{:.4f}'\n",
    "                      .format(accuracy.data), end=' ')\n",
    "\n",
    "        mi = MIEvaluator(self.encoder, self.decoder, 'cpu')\n",
    "        izx_bound = mi.eval_mi_x_z_monte_carlo(dataloader)\n",
    "        izy_bound = mi.eval_mi_y_z_variational_lb(dataloader, 10)\n",
    "\n",
    "        print('IZY:{:.2f} IZX:{:.2f}'\n",
    "                      .format(izy_bound.data, izx_bound.data))\n",
    "\n",
    "        return izx_bound.data, izy_bound.data\n",
    "\n",
    "    def parameter_counter(self):\n",
    "        a = 0\n",
    "        for p in self.encoder.parameters():\n",
    "            l = 1\n",
    "            for j in p.size():\n",
    "                l *= j\n",
    "            a = a+l\n",
    "\n",
    "        b = 0\n",
    "        for p in self.decoder.parameters():\n",
    "            l = 1\n",
    "            for j in p.size():\n",
    "                l *= j\n",
    "            b = b + l\n",
    "        return a+b\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T07:20:46.610722400Z",
     "start_time": "2024-12-24T07:20:46.602326400Z"
    }
   },
   "id": "fb6bc53273ae31a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "DataLoader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e8890949755ad55"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307), (0.3081))\n",
    "         ])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                               train=True,\n",
    "                                               transform=transform,\n",
    "                                               download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                              train=False,\n",
    "                                              transform=transform,\n",
    "                                              download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=100,\n",
    "                                              shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T07:20:46.690579100Z",
     "start_time": "2024-12-24T07:20:46.613712600Z"
    }
   },
   "id": "16ebbca2c27bc0d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compare numbers of parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "527495646df5db26"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470274\n",
      "437526\n"
     ]
    }
   ],
   "source": [
    "vib_mnist = VIB(beta=torch.tensor(10 ** 0).float())\n",
    "sib_mnist = SIB(beta=torch.tensor(10 ** 0).float()) #k=10, n=2\n",
    "print(vib_mnist.parameter_counter())\n",
    "print(sib_mnist.parameter_counter())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T07:20:46.747416400Z",
     "start_time": "2024-12-24T07:20:46.690579100Z"
    }
   },
   "id": "d38a146a1662e47a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Result of VIB"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ee0d84ab6776dc6"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.9429 IZY:2.01 IZX:3.05\n"
     ]
    }
   ],
   "source": [
    "vib_mnist.train(train_loader)\n",
    "ixz, izy = vib_mnist.fit(test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T07:24:43.995506300Z",
     "start_time": "2024-12-24T07:20:46.719510500Z"
    }
   },
   "id": "56922bfed90bbcf7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "result of SIB"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa35162f32c7d0a3"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training encoder 1\n",
      "Training encoder 2\n",
      "Train W\n",
      "acc:0.9568 IZY:2.12 IZX:2.98\n"
     ]
    }
   ],
   "source": [
    "sib_mnist.train(train_loader)\n",
    "ixz, izy = sib_mnist.fit(test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T07:37:45.047348300Z",
     "start_time": "2024-12-24T07:24:43.995506300Z"
    }
   },
   "id": "6e137191955c9d7a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
